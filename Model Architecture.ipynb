{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23597f39-cda8-4b5e-8799-62c354579759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# !pip install SciencePlots -q\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "from itertools import product\n",
    "import numpy as np \n",
    "import os, re\n",
    "from tqdm.notebook import tqdm\n",
    "from numba import njit\n",
    "from scipy.interpolate import interp1d\n",
    "from math import ceil \n",
    "from time import perf_counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def max_scaling(x):\n",
    "    x_max = x.max()\n",
    "    return x / x_max\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch import zeros, tensor, Tensor, rand\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(\"Using\", n_gpus, \"GPUs\")\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.cuda.device_count() == 1:\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(\"Using\", n_gpus, \"GPU\")    \n",
    "    device = \"cuda\"\n",
    "\n",
    "else:\n",
    "    n_gpus = 0\n",
    "    print(\"Using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "plt.style.use([\"notebook\", \"science\"])\n",
    "plt.style.use([\"notebook\", \"nature\"])\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"lines.linewidth\"] = 2\n",
    "\n",
    "np.set_printoptions(linewidth=200)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cd3e52-72a9-4e6b-b217-7f567d386be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rand(1, 80, 11)\n",
    "y = rand(1, 80, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff504bd-3cf2-4915-bd15-0848e3e08da6",
   "metadata": {},
   "source": [
    "# FNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99198c7f-220f-41ed-8c37-ec47909a7544",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FNO(nn.Module):\n",
    "    def __init__(self, hidden_size, modes, layers=1, \n",
    "                 input_size=X.shape[-1], \n",
    "                 output_size=y.shape[-1],\n",
    "        ):\n",
    "        super(FNO, self).__init__()\n",
    "\n",
    "        self.modes = min(modes, X.shape[1] // 2 + 1)\n",
    "        self.n_layers = layers\n",
    "        self.activation = F.gelu\n",
    "        \n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.p = nn.Linear(self.input_size, self.hidden_size)\n",
    "        \n",
    "        self.spectral = nn.ModuleList()\n",
    "        self.temporal = nn.ModuleList()\n",
    "        self.residual = nn.ModuleList() \n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            self.spectral += [SpectralConv1d(self.hidden_size, self.hidden_size, self.modes)]\n",
    "            self.temporal += [TemporalConv1d(self.hidden_size, self.hidden_size, self.hidden_size)]\n",
    "            self.residual += [TemporalConv1d(self.hidden_size, self.hidden_size, 1)]\n",
    "        \n",
    "        self.alpha = nn.Parameter(zeros(1))\n",
    "        self.q = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # switched place between permute and p\n",
    "        x = self.p(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            x1 = self.spectral[i](x)\n",
    "            x1 = self.temporal[i](x1)\n",
    "            x2 = self.residual[i](x)\n",
    "            \n",
    "            alpha = self.alpha.sigmoid() # 0 - 1: if alpha = 0.5 then it is equivalent to a residual layer\n",
    "            x = 2 * ((1 - alpha) * x1 + alpha * x2)\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        # switched place here aswell\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.q(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes = modes\n",
    "        self.weights = nn.Parameter(\n",
    "            zeros([1, in_channels, out_channels, self.modes], dtype=torch.cfloat)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        out_ft = torch.zeros(x.shape[0], self.out_channels, x.shape[-1] // 2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        # Batched weighted matrix multiplications\n",
    "        out_ft[...,:self.modes] = torch.einsum(\"bix, biox -> box\", x_ft[...,:self.modes], self.weights)\n",
    "        x = torch.fft.irfft(out_ft, n=x.shape[-1])\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TemporalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels):\n",
    "        super(TemporalConv1d, self).__init__()\n",
    "        \n",
    "        self.activation = F.gelu\n",
    "        \n",
    "        self.input  = nn.Conv1d(in_channels, mid_channels, 1)\n",
    "        self.conv1  = nn.Conv1d(mid_channels, mid_channels, 1)\n",
    "        self.conv2  = nn.Conv1d(mid_channels, mid_channels, 1)\n",
    "        self.output = nn.Conv1d(mid_channels, out_channels, 1)\n",
    "        \n",
    "        self.alpha1 = nn.Parameter(zeros(1))\n",
    "        self.alpha2 = nn.Parameter(zeros(1))\n",
    "        self.alpha3 = nn.Parameter(zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        \n",
    "        x1 = x\n",
    "        x2 = self.conv1(x1)\n",
    "        x2 = self.activation(x2)\n",
    "        x3 = self.conv2(x2)\n",
    "        x3 = self.activation(x3)\n",
    "        \n",
    "        x = self.alpha1.sigmoid() * x1 \\\n",
    "          + self.alpha2.sigmoid() * x2 \\\n",
    "          + self.alpha3.sigmoid() * x3\n",
    "\n",
    "        x = self.output(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0acb3-6bd6-487d-9b5f-fd6d37a3e9c6",
   "metadata": {},
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36158537-60d7-47b0-93cd-3c92bcefff2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module): \n",
    "    \n",
    "    \"\"\"\n",
    "    The padding is performed both to the left AND the right of the actual sequence. \n",
    "    However, we only need the left padding to preserve the time causal relationship.\n",
    "    To resolve this, we need to cut the convolved sequence that is double padded.\n",
    "    This is done for the last dimension as the dimension of the input tensor is transposed.\n",
    "    The size of the cut is determined by the size of the padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x[..., :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalConvBlock(nn.Module):\n",
    "    def __init__(self, input_channels, kernel_size, dilation, padding, layers_per_block, weight_init=\"default\"):\n",
    "        super(TemporalConvBlock, self).__init__()\n",
    "        \n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.activation = nn.LeakyReLU(0.01) \n",
    "        self.conv = nn.ModuleList()\n",
    "        self.chomp = nn.ModuleList()\n",
    "        self.alpha = nn.Parameter(zeros(1))\n",
    "        \n",
    "        # Weight initialization \n",
    "        if weight_init == 'default':\n",
    "            conv_weight_init = lambda w: w\n",
    "        elif weight_init == \"kaiming\" or weight_init == \"he\":\n",
    "            conv_weight_init = lambda w: nn.init.kaiming_normal_(w, nonlinearity='relu')\n",
    "        elif weight_init == 'xavier':\n",
    "            conv_weight_init = lambda w: nn.init.xavier_normal_(w, gain=nn.init.calculate_gain('relu'))\n",
    "        else:\n",
    "            raise ValueError('Invalid weight initialization method')\n",
    "\n",
    "        for i in range(self.layers_per_block):\n",
    "            conv_layer = nn.Conv1d(input_channels, input_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "            conv_weight_init(conv_layer.weight)\n",
    "            self.conv  += [weight_norm(conv_layer)]\n",
    "            self.chomp += [Chomp1d(padding)]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.layers_per_block):\n",
    "            x1 = x\n",
    "            x2 = self.conv[i](x1)\n",
    "            x2 = self.chomp[i](x2)\n",
    "            x2 = self.activation(x2)\n",
    "            alpha = self.alpha.sigmoid() # 0 to 1, 1 gievs more weight to identity map\n",
    "            x = 2 * (alpha * x1 + (1 - alpha) * x2) # x1=identity map, x2=everything\n",
    "        return x\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, hidden_size, layers_per_block=2, dilation_base=2, kernel_size=3, \n",
    "                 input_size=X.shape[-1],\n",
    "                 output_size=y.shape[-1],\n",
    "                 weight_init=\"default\",\n",
    "        ):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "\n",
    "        self.sequence_length = X.shape[1]\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dilation_base = dilation_base\n",
    "        self.number_of_layers = np.ceil(\n",
    "            np.log((self.sequence_length - 1) * (self.dilation_base - 1) / (kernel_size - 1) + 1) \n",
    "            / np.log(self.dilation_base)\n",
    "        ).astype(int)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.number_of_layers):\n",
    "            dilation_size = self.dilation_base ** i # this is a generalized dilation rate\n",
    "            self.layers += [TemporalConvBlock(self.hidden_size, kernel_size, \n",
    "                                              dilation=dilation_size, \n",
    "                                              layers_per_block=layers_per_block,\n",
    "                                              padding=(kernel_size-1) * dilation_size,\n",
    "                                              weight_init=weight_init,\n",
    "                                             )]\n",
    "            self.layers += [nn.BatchNorm1d(num_features=self.hidden_size)]\n",
    "            \n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "        self.p = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.q = nn.Linear(self.hidden_size, self.output_size)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.p(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.network(x) \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.q(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617a18d-f293-4d0a-bb4b-9f4a4f70e03d",
   "metadata": {},
   "source": [
    "# TFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da00589e-f43a-4e8e-89a7-bc1eea879676",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFN(nn.Module):\n",
    "    def __init__(self, hidden_size, modes=np.infty, \n",
    "                 layers=1, n_quantiles=1, \n",
    "                 input_size=X.shape[-1],\n",
    "                 output_size=y.shape[-1],\n",
    "        ):\n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.modes = min(modes, X.shape[1] // 2 + 1)\n",
    "        self.activation = F.gelu\n",
    "        self.n_quantiles = n_quantiles\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size * self.n_quantiles\n",
    "        \n",
    "        self.FNO = FNO(self.hidden_size, self.modes, layers, input_size=self.input_size, output_size=self.output_size)\n",
    "        self.TCN = TCN(self.hidden_size, 2 ,2, 3, input_size=self.input_size, output_size=self.output_size)\n",
    "        \n",
    "        self.alpha = nn.Parameter(zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        TCN_predictions = self.TCN(x)\n",
    "        FNO_predictions = self.FNO(x)\n",
    "        \n",
    "        alpha = self.alpha.sigmoid() # 0 < alpha < 1, initialized at alpha = 0.5, 0 means that the temporal domain is prefered over the frequency counterpart \n",
    "        TFN_predictions = alpha * FNO_predictions + (1 - alpha) * TCN_predictions\n",
    "        quantile_predictions = TFN_predictions.view(\n",
    "            *x.shape[:-1], self.output_size // self.n_quantiles, self.n_quantiles\n",
    "        ).contiguous()\n",
    "        \n",
    "        return quantile_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7de4f-b354-43eb-b3b8-10093ca810c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f65dc-6250-46d7-8f7f-863112763d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55dae5-fa10-477c-bf48-6fb9fe7bb5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
